# 參考文獻筆記

**整理日期**: 2025-11-27  
**專案**: 交通號誌控制方法比較實驗

---

## 目錄

1. [Webster Method](#1-webster-method)
2. [Proximal Policy Optimization (PPO)](#2-proximal-policy-optimization-ppo)
3. [Max Pressure Control](#3-max-pressure-control)
4. [Large Language Models as Traffic Signal Control Agents](#4-large-language-models-as-traffic-signal-control-agents)

---

## 1. Webster Method

**論文資訊**:
- **標題**: Networked Sensor-Based Adaptive Traffic Signal Control for Dynamic Flow Optimization
- **期刊**: sensors-25-03501
- **原始方法**: Webster, F.V., "Traffic Signal Settings", Road Research Technical Paper No. 39, 1958
- **類型**: 固定時制交通號誌控制

---

### 1. 方法定位與基本精神

Webster (1958) 是交通工程中最經典、最廣泛使用的
**固定時制（Fixed-time）**號誌配時公式。

**其目標**:
- 設計一個**最佳週期時間（optimal cycle length）**
- 根據交通流量分配**綠燈時間**

**核心假設**:
- 車流量為**穩定、可預測**
- 適用於**中低需求**或**離峰 / 不變動流量**
- 不針對即時狀態做反應（非 adaptive）

**sensors-25-03501 論文**:
- 採用 SUMO 作為模擬器
- 完整實作 Webster Cycle + Green Split
- 並比較與 AI / 優化方法的差異

---

### 2. Webster 方法的三大核心公式

Webster 方法由三個核心步驟構成：

#### Step 1 — 計算「最適週期」C*

**Webster Optimal Cycle Length Formula**:

```
C* = (1.5L + 5) / (1 - Y)
```

其中：
- **L**: 總損失時間（lost time）
  - 一般 = 所有相位的黃燈 + 起步損失
  - sensors 論文中 L 根據 SUMO 的 phase 設定得出
- **Y = Σ y_i**: 所有方向的 flow ratio 之總和
- **y_i = q_i / s_i**
  - q = 流量（veh/s or veh/h）
  - s = 飽和流率（通常取 1800 veh/h）

**C* 通常會被取為**:
- 上限：120～180 秒
- 下限：60 秒

論文中也採取此慣例。

#### Step 2 — 計算 Green Split（各相位綠燈時間）

Webster 的綠燈分配（split）公式：

```
g_i = (y_i / Y) × (C* - L)
```

其中：
- **g_i**: 相位 i 的綠燈秒數
- **y_i**: 該相位的 flow ratio
- **C***: 上一步計算出的最佳週期
- **L**: 總損失時間（含黃燈）

sensors 論文也依此公式計算每一個 phase 的綠燈比例。

#### Step 3 — 設定最終 phase durations

每一個 phase 包含：

```
[green] → [yellow] → [red clearance]
```

sensors 論文中：
- 綠燈 = Webster 的 g_i
- 黃燈 = SUMO default (如 3 sec)
- all-red / clearance time = 依 SUMO 交叉口幾何自動產生

---

### 3. 論文中的 Webster 實作流程（完整重建版）

根據 sensors-25-03501，完整步驟如下：

#### Step 0 — 收集交通流量

論文在 SUMO 中放置 detectors，收集：
- 車流量（veh/h）
- 飽和流率（設定為固定值）
- 各方向流量 q_i

這是 Webster 需用到的唯一輸入（非即時）。

#### Step 1 — 計算 flow ratio

```
y_i = q_i / s_i
```

- 飽和流率 s_i 固定為 1800 veh/h
- q_i 由 SUMO 模擬（或手動設置）

#### Step 2 — 求和所有 flow ratio

```
Y = Σ y_i
```

#### Step 3 — Loss time 設定

論文使用：
- 黃燈時間：由 SUMO 設定（如 3s）
- 起步損失：一般假設 2s（或在 SUMO 中自然產生）
- Phase 數：根據交叉口結構（通常 2 or 4 phases）

損失時間為：

```
L = (# of phases) × (黃燈 + 起步損失)
```

#### Step 4 — 計算最佳週期（C*）

```
C* = (1.5L + 5) / (1 - Y)
```

論文指出：
- Y 高時（壅塞） → C* 長
- Y 低時（離峰） → C* 短

#### Step 5 — 計算各相位綠燈時間 g_i

```
g_i = (y_i / Y) × (C* - L)
```

#### Step 6 — SUMO 時相自動產生紅燈時間

SUMO 自動補齊：
- all-red
- amber to red clearance

最終得到：

```
[綠燈 g_i] → [黃燈] → [紅燈]
```

這一組完整相位即 Webster 固定時制。

#### Step 7 — 固定循環執行

sensors 論文中的 Webster 是：
- ✔ 不會因車流動態變動而調整
- ✔ 一個 C* 固定跑完整模擬
- ✔ 不含自適應（非 actuated）

---

### 4. 論文中的實驗設置（來自 sensors-25-03501）

#### 4.1 使用模擬器

- SUMO
- 採用 TraCI 讀取流量
- 各相位根據 Webster 計算固定 duration

#### 4.2 評估指標

和實驗報告一致：
- Average Queue Length
- Delay Time
- Number of stopped vehicles
- Travel time
- Throughput

**Webster 通常**:
- 在高車流情境 → 表現最差
- 在穩定、可預測車流 → 表現合理（但不如 adaptive）

---

### 5. sensors 論文對 Webster 方法的結論（技術客觀整理）

論文指出：

#### 優點

- 結構簡單、可解釋
- 固定時制 → 適合中低需求
- 是傳統交通工程基準方法
- 不需要任何即時感測資料

#### 限制（也是實驗中看到的）

- 車流動態變化時無法調整（非 adaptive）
- 高流量狀況下排隊會迅速累積
- 無法處理尖峰流量
- 可能造成左轉/小流量方向排隊過長
- 完全沒有 RL 或 MaxPressure 的「壓力平衡」能力

sensors 論文也因此指出：

> Webster 方法在高需求模擬中的表現顯著劣於智能控制方法。

這與實驗結果完全相符。

---

### 6. 總整理

Webster 方法是一種**固定時制交通號誌控制方法**，
由三個主要公式構成：

#### 1. 最佳週期公式（Optimal Cycle）

```
C* = (1.5L + 5) / (1 - Y)
```

#### 2. flow ratio

```
y_i = q_i / s_i
```

#### 3. green split

```
g_i = (y_i / Y) × (C* - L)
```

**sensors-25-03501 論文中使用 SUMO 重現 Webster**，流程如下：

1. 收集流量
2. 計算 flow ratio
3. 設定損失時間
4. 計算 C*
5. 計算綠燈 g_i
6. SUMO 自動加黃燈與紅燈
7. 以固定 cycle 執行整場模擬

因此 Webster 方法是一種**可預測但不自適應**的號誌控制方式，
在穩定流量下有效，但在高動態或尖峰車流狀態下顯著劣化。

---


## 3. Max Pressure Control

**論文資訊**:
- **標題**: Max pressure control of a network of signalized intersections
- **期刊**: Transportation Research Part C
- **年份**: 2013
- **作者**: Pravin Varaiya
- **類型**: 交通號誌控制理論

---

### 1. 方法定位與核心概念

**Max Pressure（最大壓力控制）**是一種非參數化、即時、自適應的交通號誌控制方法。

其核心思想來自於：

如果把每個相位當成是在「把車流往下游送」的機制，那麼應該優先讓那些「上游車很多、下游快空了」的相位放行。

也就是說：
- 上游排隊越多 → 越需要放行
- 下游越空 → 越適合放行（避免下游阻塞）
- 綜合兩者差值 → 「壓力」

**Max Pressure 的重點**:
- 不需要校正參數
- 不依賴需求預測
- 能保證全網路穩定性（network stability）
- 計算量低，非常適合即時控制

它是目前交通訊號控制理論中證明最完整且表現最穩健的 adaptive 方法之一。

---

### 2. 基本數學架構與符號

對每個 intersection i，Max Pressure 依賴：

#### 2.1. 車道 queue

令 **Q_ab**：從入口 a 到出口 b 的 queue 長度（vehicles）

其中 a、b 為車道或車流方向。

#### 2.2. movement（轉向 movement）

每個 movement a→b 對應到一個控制方向，例如：
- EB → 東往西直行
- EB_left → 東往西左轉
- SB → 南往北直行

#### 2.3. 相位（Phase）

相位是由一組允許同時進行的 movements 所組成。
例如：
- Phase 0：EB 直行
- Phase 1：SB 直行

每個 phase p 有若干 movements M_p。

---

### 3. 壓力（Pressure）的定義

Varaiya 給出的 Max Pressure 定義如下：

#### Movement Pressure

```
P_ab = Q_ab - Σ_c p_bc * Q_bc
```

其中：
- **Q_ab**: 上游 a 的 queue
- **Σ_c p_bc * Q_bc**: 下游方向 b 的 queue
- **p_bc**: 車輛從 b 開往 c 的轉向比例（turning ratio）

**直覺**:
- 上游越塞 → 增加 pressure
- 下游越塞 → 降低 pressure（因為送車過去會造成壅塞）

#### Phase Pressure（相位壓力）

每個相位 p 的壓力為其所有 movement 壓力總和：

```
P(p) = Σ_{(a,b) ∈ M_p} s_ab · P_ab
```

其中 s_ab 為飽和流率或容量因子（常取 1）。

#### Max Pressure 控制決策：

```
p* = argmax_p P(p)
```

永遠選壓力最高的相位，持續給綠燈。

---

### 4. 控制演算法流程（Varaiya 2013 標準版）

這是原論文的完整決策步驟，不含任何額外 heuristics。

#### Step 1：蒐集 queue 資訊

對所有 movements a→b 讀取：
- 上游 queue：Q_ab
- 下游 queue：所有 Q_bc

資料可來自：
- 感測器
- 車輛偵測器
- 模擬器（如 SUMO）

#### Step 2：計算 movement pressure

對每個 movement：

```
P_ab = Q_ab - Σ_c p_bc * Q_bc
```

如果未提供 turning ratios，許多實作中直接簡化為：

```
P_ab = Q_ab_upstream - Q_b_downstream
```

#### Step 3：計算 phase pressure

對每個可選相位：

```
P(p) = Σ_{(a,b) ∈ M_p} P_ab
```

#### Step 4：選擇壓力最大的相位

```
p* = argmax_p P(p)
```

#### Step 5：保持該相位直到下一次決策

Max Pressure 沒有固定綠燈時間要求；
論文中一般在**每個小時隙（slotted time）**執行一次決策，例如：
- 每 5 秒
- 每 10 秒

下一次到達決策點又重新計算 pressure。

#### Step 6：重複

整體流程可寫成 pseudocode：

```python
Loop every Δ seconds:

    For each movement (a→b):
        Compute P_ab = Q_ab − Σ p_bc * Q_bc

    For each phase p:
        Compute P(p) = Σ_{(a,b) in M_p} P_ab

    Select phase p* = argmax P(p)

    Activate phase p* for next Δ seconds
```

---

### 5. Max Pressure 理論保證（Varaiya 的最大貢獻）

Varaiya 2013 的最大理論成果：

#### Theorem（核心穩定性定理）

若流量落在網路的可穩定區域內，
則 Max Pressure 控制能保證：
- 整個路網 queue 有界（bounded queues）
- 最大 throughput
- 網路全局穩定性（global stability）

這是目前交通號誌控制演算法中少見的強數學保證。

#### 比較：

| 方法 | 是否有穩定性證明？ |
|------|-------------------|
| Fixed-Time | ❌ 無（需人工調整） |
| Webster | ❌ 無全局穩定性證明 |
| RL（DQN, PPO…） | ❌ 無理論穩定性保證 |
| **Max Pressure** | ✔ 有數學證明 |

因此 Max Pressure 在理論界被視為最具代表性的 adaptive 控制方法。

---

### 6. Max Pressure 在交通號誌控制中的映射（一般論述）

#### 6.1 State

包含所有 queue：
- 上游 queue
- 下游 queue
- （可選）turning ratio

#### 6.2 Action

選擇下一個相位：
- Phase 0
- Phase 1
- Phase 2
- Phase 3

沒有傳統意義上的 duration 決策；
綠燈時間由 decision interval 決定（例如固定 10 秒）。

#### 6.3 Reward（非論文概念）

Max Pressure 不使用 reward（它不是 RL）。
壓力就是它的決策依據。

---

### 7. 設計上的優勢

Varaiya 論文強調 Max Pressure 有以下特性：

#### 無須流量預測
不需要 demand estimation。

#### 不需 tuning
沒有參數需要校正（turning ratios 可用估計值）。

#### 高穩定性
有嚴謹的 Lyapunov-based 穩定性證明。

#### 適合高壅塞狀態
在高需求下表現尤佳（比固定時制明顯好）。

#### 即時適應
每次 decision interval 重新計算壓力 → 自動調整配時。

---

### 8. 方法限制（客觀整理）

Max Pressure 也有幾項公認限制（論文與後續研究一致）：

#### 需要 queue 資料
如果沒有偵測器，需要模擬器才能運作。

#### 不包含黃燈/安全切換邏輯
需要額外實作（例如 0→1→2 類的 intermediate phase）。

#### 無法主動做綠波協調
只看 local queue → 無跨路口協同。

#### 可能產生過度頻繁切換
尤其 decision interval 很短時。

---

### 9. 小結（客觀版）

Max Pressure 是 Varaiya 提出的具有強數學基礎的交通號誌控制方法。

其核心決策原理是：
**選擇「上游車多、下游車少」的相位，使整體流動性最大化。**

**其優勢包括**:
- 理論保證網路穩定
- 不需要參數調校或預測
- 能應對壅塞並即時自適應
- 計算量極低、實務部署容易

**其限制主要在於**:
- 需要可靠 queue 資料
- 不自動處理黃燈與安全過渡
- 僅 local 最佳，無 global 協調
- 若 decision interval 太短，可能過度切換相位

Max Pressure 至今仍是自適應號誌控制中最具代表性、最穩定、最合理的演算法之一。

---


## 2. Proximal Policy Optimization (PPO)

**論文資訊**:
- **標題**: Proximal Policy Optimization Algorithms
- **arXiv**: 1707.06347
- **作者**: John Schulman et al. (OpenAI)
- **類型**: 強化學習演算法

---

### 1. 方法定位與核心想法

PPO 是一種 **on-policy policy gradient** 強化學習方法。

**目標**: 在更新 policy 時「走大步又不要走過頭」。

**背景問題**: 傳統 Policy Gradient 或 TRPO（Trust Region Policy Optimization）要嘛：
- 更新太大步 → 表現突然變差、發散
- 或像 TRPO 一樣需要複雜的二階優化（計算成本高）

**PPO 的核心設計**:
用一個「裁切過的目標函數 (clipped objective)」來近似 trust region 的效果，
在不大幅增加計算量的情況下，控制每次 policy 更新的幅度。

---

### 2. 基本符號與設定

給定一個參數化的 policy π_θ(a|s)：

- **θ**: policy 參數
- **s_t, a_t, r_t**: 在環境中收集到的 state、action、reward
- **V_φ(s)**: value function（critic），參數為 φ

為了穩定更新，PPO 引入**舊 policy π_θ_old**，
並定義**機率比 (ratio)**：

```
r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)
```

同時定義**優勢函數 (advantage) Â_t**，描述「這個 action 比平均好多少」。

---

### 3. Clip Objective 公式（PPO-Clip）

PPO 最常用的版本是 **PPO-Clip**。
其 policy loss 定義為：

```
L_CLIP(θ) = E_t[min(r_t(θ)Â_t, clip(r_t(θ), 1-ε, 1+ε)Â_t)]
```

其中：
- `r_t(θ)Â_t`: 一般 policy gradient 的目標
- `clip(r_t(θ), 1-ε, 1+ε)`: 把 ratio 限制在 [1-ε, 1+ε]
- **ε**: 一個小常數（例如 0.1 或 0.2）

**直覺**:
- 如果 r_t(θ) 偏離 1 太多（代表新 policy 與舊 policy 差異太大），clip 會「修剪」梯度，避免過度更新。
- Loss 取兩者的最小值（對優勢為正時），保證 policy 不會在提升 advantage 的同時走太遠。

---

### 4. Advantage 與 GAE（Generalized Advantage Estimation）

PPO 需要一個平滑、低方差的優勢估計 Â_t。
論文採用 **GAE(λ)**：

#### 定義 TD 誤差：
```
δ_t = r_t + γV_φ(s_{t+1}) - V_φ(s_t)
```

#### 將多步 TD 誤差加權疊加：
```
Â_t = Σ(γλ)^l δ_{t+l}  (l=0 to ∞)
```

- **γ**: 折扣因子
- **λ**: 控制 bias / variance 的平衡（0~1）

**GAE 的作用**:
在 variance 和 bias 之間取得折衷，使 advantage 比單步 TD 或 Monte Carlo 更穩定。

---

### 5. PPO 演算法流程（標準版偽程式）

以原始論文的 on-policy、PPO-Clip 型式為例：

```python
Initialize policy parameters θ
Initialize value function parameters φ

for iteration = 1, 2, ... do:
    # 1. 蒐集資料（rollouts）
    Run policy π_θ_old in environment for T timesteps
    Collect trajectories: (s_t, a_t, r_t, ...)

    # 2. 計算回報與 advantage
    Compute V_φ(s_t) for all states
    Compute advantage Â_t using GAE(λ)
    Compute returns R_t (折扣回報)

    # 3. Policy 更新 (多個 epoch)
    For K epochs:
        - 以 mini-batch 方式抽樣 {s_t, a_t, Â_t}
        - 計算機率比 r_t(θ)
        - 計算 L_clip(θ)
        - 以梯度上升 / 降低 -L_clip(θ) 更新 θ

    # 4. Value function 更新
    Optimize value loss:
        L_v(φ) = E_t[(V_φ(s_t) - R_t)²]

    # 5. 更新舊 policy 參數
    θ_old ← θ
end for
```

**關鍵特徵**:
- **On-policy**: 每次迭代都用最新 policy 去收集資料
- **多 epoch 更新**: 同一批資料可以用多次（例如 K = 10），提高 sample efficiency
- **Clipping**: 在 policy loss 中控制更新步長
- **Actor-Critic**: policy 與 value function 同時訓練

---

### 6. 損失函數組成

完整的 PPO objective 通常包含三部分：

#### 1. Policy Loss (Clip)
```
L_CLIP(θ)
```

#### 2. Value Loss
```
L_V(φ) = E_t[(V_φ(s_t) - R_t)²]
```

#### 3. Entropy Bonus（可選，鼓勵探索）
```
L_S(θ) = E_t[β·H(π_θ(·|s_t))]
```

**總 loss 一般寫成**:
```
L(θ,φ) = E_t[L_CLIP(θ) - c₁L_V(φ) + c₂L_S(θ)]
```

- **c₁, c₂**: 權重超參數
- **H**: policy 的熵，鼓勵在 early stage 保持隨機性

---

### 7. 在交通號誌控制中的典型映射（一般性說明）

當 PPO 應用在**交通號誌控制 (Traffic Signal Control, TSC)** 時，
一般會做出如下抽象映射（不依賴特定實作）：

#### 7.1 狀態 s_t

可包含（例）：
- 各方向的 queue length（每條車道或每方向）
- 停等車輛數、平均速度等
- 當前相位編號
- 當前相位剩餘或已經持續的時間
- 其他感測器數據（如延滯時間估計）

**目標**: 提供足夠資訊讓 agent 判斷「哪個方向壓力最大」。

#### 7.2 動作 a_t

常見兩類設計：

**離散動作**: 選擇下一個號誌相位
- 例如：phase ∈ {東西直行、南北直行、左轉相位…}
- 每次決策：在固定 decision interval 時點，選擇下個 phase

**連續或混合動作**: 決定綠燈時間長度
- 給出一個 Δt 或直接給出該 phase 的 green time
- 搭配最小/最大綠燈約束進行裁切

#### 7.3 reward r_t

典型設計是讓 reward 與「壅塞程度」反向關聯，例如：
- `r_t = -`（總延滯時間）
- `r_t = -`（總停等車輛數）
- `r_t = -`（加權後的 queue length）
- 或把 throughput 也放入 reward

**核心思想**:
- reward 越大 → 壅塞越少
- agent 就會學出「降低延滯 / queue」的控制策略。

---

### 8. 超參數與實務注意事項（論文角度）

論文與後續工作中常見的 PPO 設定：

- **折扣因子 γ**: 0.99 或 0.995
- **GAE 參數 λ**: 0.95
- **Clip 係數 ε**: 0.1 ~ 0.2
- **每次迭代樣本數**: 例如 2048 / 4096 steps
- **mini-batch 大小**: 64 / 128
- **policy 更新 epoch**: 3 ~ 10
- **optimizer**: Adam，learning rate 約 3e-4 左右

**實務上 PPO 的穩定性相對較好**，但仍需要：
- 合理的 reward 設計
- 不要讓 state/action 維度過大但沒有對應的訓練量
- 避免過度更新（Clip、小 learning rate 共用）

---

### 9. 小結（純客觀整理）

PPO 是一種 **on-policy actor-critic** 演算法，
核心是使用 **clip objective** 控制 policy 更新幅度。

以**機率比 r_t(θ)** 和 **GAE advantage** 為基礎，
兼具 sample efficiency 與訓練穩定性。

在交通號誌控制場景中，
可以自然地把「交通狀態」映射到 state，
把「相位/綠燈時間」映射到 action，
以「延滯、queue、停車」設計 reward。

相較於傳統固定時制或手工設計規則，
PPO 可以自動從模擬中學得一套 policy，
在高需求或複雜車流下具有較佳的適應性與表現（這也是多篇 TSC-RL 文獻採用 PPO 的原因）。

---


## 4. Large Language Models as Traffic Signal Control Agents

**論文資訊**:
- **標題**: Large Language Models as Traffic Signal Control Agents
- **arXiv**: 2312.16044v5
- **類型**: LLM 應用於交通號誌控制

---

### 1. 前言（Introduction）

這篇論文提出一種免訓練、由 LLM 驅動的交通號誌控制器。
與強化學習（RL）或傳統交通工程公式不同，這個控制器使用：

- 以自然語言編碼的交通狀態
- LLM 的推理（分析 + chain-of-thought）
- LLM 產生的控制動作（相位 + 持續時間）

這種方法把交通號誌控制器（TSC）變成一個 LLM agent，
透過高階推理來做決策，而不是依賴學習出的 Q-value 或 policy network。

---

### 2. 系統架構（System Architecture）

這個 LLM 型交通號誌控制器包含四個主要模組：

```
Traffic Simulator → State Encoder → LLM Decision Module → Action Executor
```

#### 2.1 狀態編碼器（State Encoder）

把原始交通資訊轉換成自然語言描述，包括：

- 各方向車道的排隊長度（queue length）
- 流量（每小時車輛數，flow rate）
- 目前號誌相位
- 當前相位已經持續的時間

**範例**:
```
Northbound queue: 12 cars
Southbound queue: 8 cars
Eastbound queue: 3 cars
Westbound queue: 6 cars
Current phase: East-West straight
Elapsed: 24 seconds
```

#### 2.2 LLM 決策模組（LLM Decision Module）

在給定狀態描述與精心設計的 prompt 之後，
LLM（例如 GPT-4、GPT-4-turbo）輸出：

```
Thought:（推理過程…）
Action: {"phase": "NS_straight", "duration": 30}
```

它會執行：
- 交通狀態分析
- 各候選相位的比較
- 選擇最佳相位
- 建議綠燈持續時間

#### 2.3 動作解析器（Action Parser）

從 LLM 輸出中抽取可執行的控制指令：

- `"phase"` → 對應到模擬器中的相位索引
- `"duration"` → 被限制（clamp）在安全範圍（10–60 秒）

#### 2.4 號誌執行器（Traffic Signal Executor）

依照 LLM 選定的相位與時間執行控制：

1. 切換到指定相位
2. 維持該相位 N 秒
3. 推進模擬時間
4. 重複流程

---

### 3. 問題定義（Problem Formulation）

**目標**: 控制單一路口（之後可擴展到多路口），使下列指標最小化：

- 排隊長度（queue length）
- 車輛延滯（vehicle delay）
- 行程時間（travel time）
- 壅塞擴散（congestion propagation）

**傳統 RL 方法**:
```
state → neural policy → action
```

**本論文方法**:
```
state（文字）→ LLM 推理 → action（文字 → 控制命令）
```

不需要額外訓練；只依賴 prompt engineering 下的 LLM 推論即可。

---

### 4. 狀態表示（文字化 State Representation）

狀態使用一個結構化的自然語言模板來表示：

```
Traffic Condition:
Northbound: queue={N_q}, flow={N_f}
Southbound: queue={S_q}, flow={S_f}
Eastbound:  queue={E_q}, flow={E_f}
Westbound:  queue={W_q}, flow={W_f}

Current phase: {phase_name}
Elapsed: {elapsed}s
```

這樣將原本的數值輸入轉成 LLM 容易理解、適合推理的文字描述。

---

### 5. 動作空間（Action Space）

LLM 從四個標準號誌相位中選擇其一：

- `NS_straight`
- `EW_straight`
- `NS_left`
- `EW_left`

同時 LLM 也會輸出 10–60 秒之間的一個綠燈時間。

**輸出格式強制為 JSON**:
```json
{
  "phase": "NS_straight",
  "duration": 30
}
```

為避免 hallucination，若 phase 無效，則會退回到安全的備援策略。

---

### 6. Prompt Engineering

Prompt 設計是這篇論文的核心。
作者提出兩種方法：

#### 6.1 直接 Prompt（Direct Prompting, DP）

簡單版形式：

```
給定交通排隊狀況：
N={N_q}, S={S_q}, E={E_q}, W={W_q}

選擇下一個號誌相位與綠燈時間，
並只輸出 JSON。
```

在低流量狀況下表現尚可，但在壅塞狀態下較弱。

#### 6.2 Reasoning + Action Prompt（RAP）

**本論文的主要貢獻**。
它強迫 LLM 先推理，再決策。

```
You are an intelligent traffic signal controller.

Goal: minimize queue length and delay.

Step 1: analyze traffic.
Step 2: compare each possible phase.
Step 3: choose the best phase and duration.

Output format:
Thought: <step-by-step reasoning>
Action: {"phase": "...", "duration": ...}
```

這大幅提升了穩定性與一致性。

---

### 7. LLM 決策流程（技術細節）

以下是論文中 LLM 控制演算法的概念性 pseudocode：

```python
def LLM_controller(state):
    prompt = build_prompt(state)

    response = openai.ChatCompletion.create(
        model="gpt-4-turbo",
        messages=[{"role": "user", "content": prompt}]
    )

    parsed = extract_json(response["choices"][0]["message"]["content"])

    # Safety checks
    phase = validate_phase(parsed["phase"])
    duration = clamp(parsed["duration"], 10, 60)

    return phase, duration
```

---

### 8. 安全性與有效性機制（Safety and Validity Mechanisms）

LLM 的輸出在未檢查前是不安全的。
論文加上了以下保護機制：

#### JSON 格式檢查
確保 LLM 輸出的確是合法 JSON。

#### 相位白名單（Phase whitelist）
只接受像：
```json
{"phase": "NS_straight"}
```
其餘一律拒絕。

#### 綠燈時間限制
將 duration 限制在 10–60 秒之間。

#### 備援控制器（Fallback controller）
若輸出格式錯誤 → 退回 Max-Pressure 或固定時制控制。

#### Hallucination 過濾
拒絕未知的相位名稱或奇怪的單位。

這些機制確保系統在即時控制時仍能保持穩定。

---

### 9. 完整控制迴圈（可重現層級細節）

```python
while simulation_running:

    # 1. 讀取狀態
    state = {
        "queue": get_queue_lengths(),
        "flow": get_flow(),
        "current_phase": get_phase(),
        "elapsed": get_elapsed_time(),
    }

    # 2. 建構 prompt 並查詢 LLM
    phase, duration = LLM_controller(state)

    # 3. 執行相位
    set_phase(phase)
    for _ in range(duration):
        traci.simulationStep()
```

這就是論文在 CityFlow 中使用的完整控制流程。

---

### 10. 與 SUMO / CityFlow 的整合

#### 在 SUMO（你的情境）中，相位映射如下：

```python
phase_map = {
    "NS_straight": 0,
    "NS_left":     1,
    "EW_straight": 2,
    "EW_left":     3,
}
```

#### 狀態讀取方式：

```python
queue = traci.lane.getLastStepHaltingNumber(lane)
flow  = traci.lane.getLastStepVehicleNumber(lane)
phase = traci.trafficlight.getPhase("Node2")
```

#### 執行控制：

```python
traci.trafficlight.setPhase("Node2", phase_map[action_phase])
step_simulation_for(duration)
```

---

### 11. 實驗設計（Experiment Setup）

論文將 LLM-TSC 與以下方法比較：

- Fixed-Time（固定時制）
- Max-Pressure
- RL 模型（CoLight、MPLight、PressLight）
- LLM (DP)
- LLM (RAP)
- LLM 持續學習（Continual Learning）

**評估指標包括**:
- 平均排隊長度（Average Queue Length）
- 延滯時間（Delay Time）
- 總旅行時間（Total Travel Time）
- 通行量（Throughput）

**結果顯示**，LLM RAP 的表現為：
- 在低流量時，接近 RL 模型
- 在高流量時更穩定
- 在不同路口間具有較好的泛化能力

---

### 12. 優點與技術洞見（Strengths and Technical Insights）

#### 免訓練（Training-free）
LLM 不需要 RL 訓練，計算成本可比 RL 少上 10,000 倍。

#### 自然語言彈性
可以對從未見過的交通狀態進行推理。

#### 可解釋性
LLM 會輸出 Thought，讓決策過程透明可讀。

#### 持續調整能力（Continual adaptation）
只要改 prompt 就能調整行為，不需要重新訓練模型。

---

### 13. 限制（Limitations）

- LLM 推論延遲（約 1–2 秒）可能成為瓶頸
- 缺乏全局規劃（只做單路口局部決策）
- 仰賴結構化 prompt 的正確性
- 必須有強力的安全防護來避免 hallucination

---

### 14. 完整 Prompt 範本（可直接使用）

```
You are a traffic signal controller for an urban intersection.
Your objective is to reduce congestion, queue length, and delay.

Traffic Condition:
Northbound: queue={N_q}, flow={N_f}
Southbound: queue={S_q}, flow={S_f}
Eastbound:  queue={E_q}, flow={E_f}
Westbound:  queue={W_q}, flow={W_f}
Current phase: {phase_name}
Elapsed time: {elapsed}s

Available phases:
[NS_straight, NS_left, EW_straight, EW_left]
Duration must be between 10 and 60 seconds.

Think step-by-step.
Analyze the traffic.
Compare all possible phases.
Output your final decision strictly in JSON:

{
  "phase": "...",
  "duration": ...
}
```

---

### 15. 結論（Conclusion）

這篇論文顯示，LLM 可以作為一種免訓練的交通號誌控制器，
在不需要 RL 訓練的情況下，就能達到與 RL 模型相近的表現，
同時提供可解釋的推理過程。

上述重現步驟可以完整實作於：
- SUMO（透過 TraCI）
- CityFlow

---

**文件結束**
